{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritkumar17/Desktop/R&D_Project/Agentic_AI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from agentic_ai.src.decision_making_agent_prompt import custom_llm_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/rndss5vd6h9cdr24fwvnz6kw0000gp/T/ipykernel_68767/341949512.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"mps\")\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/00/rndss5vd6h9cdr24fwvnz6kw0000gp/T/ipykernel_68767/1255448939.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  knowledge_base = Chroma(persist_directory=path, embedding_function=embeddings_model)\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/ritkumar17/Desktop/R&D_Project/Agentic_AI/agentic_ai/data/chroma_db\"\n",
    "knowledge_base = Chroma(persist_directory=path, embedding_function=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = knowledge_base.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.70s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n",
      "/Users/ritkumar17/Desktop/R&D_Project/Agentic_AI/agentic_ai/src/decision_making_agent_prompt.py:37: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  custom_llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "llm = custom_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Have we done anything on Autonomous Vehicle Navigation System\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import create_stuff_documents_chain, create_retrieval_chain\n",
    "\n",
    "# # Mistral-specific prompt template\n",
    "# mistral_template = \"\"\"[INST] \n",
    "# <<SYS>>\n",
    "# You are a helpful assistant for Company XYZ. Answer strictly based on these documents:\n",
    "# {context}\n",
    "# <</SYS>>\n",
    "\n",
    "# {input} \n",
    "# [/INST]\"\"\"  # Closing tag must be at end of prompt\n",
    "\n",
    "# # Create standard PromptTemplate (not ChatPromptTemplate)\n",
    "# prompt = PromptTemplate.from_template(mistral_template)\n",
    "\n",
    "# # Create document chain\n",
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# # Create retrieval chain\n",
    "# chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# # Invoke with input\n",
    "# response = chain.invoke({\"input\": \"What is the project deadline?\"})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Have we done anything on Autonomous Vehicle Navigation System',\n",
       " 'context': [Document(metadata={}, page_content='\\n\\nProject Titan: Autonomous Vehicle Navigation System\\nProject Overview:\\nProject Titan was focused on developing a robust autonomous driving system for self-driving cars. The primary objective was to navigate complex urban environments while ensuring the safety of passengers and pedestrians. The system needed to process data from cameras, LIDAR, and radar sensors in real time to make driving decisions.\\n\\nAlgorithms Tried:\\nWe experimented with classical computer vision techniques like Convolutional Neural Networks (CNNs) for object detection and lane detection. Deep Reinforcement Learning (DRL) was also tested for decision-making. Ultimately, a combination of CNNs for vision and DRL for control actions provided the best results.\\n\\nBest Performing Algorithm:\\nA hybrid model combining CNNs for object detection and Deep Reinforcement Learning (DRL) for navigation outperformed all other approaches. This setup allowed the car to learn optimal driving strategies through real-time feedback loops.\\n\\nKey Metrics:\\nCollision Avoidance Rate: 98%\\nLane-Keeping Accuracy: 96%\\nAverage Decision Latency: 50ms\\nNext Steps:\\nWe plan to expand the model to handle more complex weather conditions and integrate Vehicle-to-Vehicle (V2V) communication to improve safety and navigation in dense traffic environments.\\n\\n'),\n",
       "  Document(metadata={}, page_content='\\n\\nProject Argus: Real-Time Traffic Prediction for Smart Cities\\nProject Overview:\\nProject Argus focused on developing a real-time traffic prediction system for smart cities. The objective was to reduce congestion by predicting traffic flow on major routes and suggesting alternative paths for commuters. The project used data from GPS devices, road sensors, and historical traffic data to generate accurate traffic predictions.\\n\\nAlgorithms Tried:\\nWe started with classical time-series models like ARIMA and moved to more sophisticated approaches like Gradient Boosting Machines and Random Forests for feature-rich data. However, the breakthrough came with Graph Neural Networks (GNNs), which could model the spatial relationships between different road networks effectively.\\n\\nBest Performing Algorithm:\\nGraph Neural Networks (GNNs) outperformed all other models, as they were able to capture both spatial and temporal dependencies. GNNs modeled the interconnectedness of road networks while simultaneously factoring in historical traffic patterns.\\n\\nKey Metrics:\\nPrediction Accuracy: 92%\\nAverage Delay Reduction: 15%\\nFuel Consumption Reduction: 10%\\nNext Steps:\\nNext steps include extending the system to incorporate weather data and accident reports for more comprehensive traffic predictions. We also plan to integrate this system into city-wide traffic management systems to allow for automated signal adjustments based on real-time traffic conditions.\\n\\n'),\n",
       "  Document(metadata={}, page_content='\\n\\nProject Mercury: Real-Time Ad Optimization for Digital Marketing\\nProject Overview:\\nProject Mercury focused on building an AI-driven ad optimization engine for a digital marketing platform. The goal was to maximize return on ad spend (ROAS) by dynamically adjusting ad placements, targeting, and bids in real time based on user behavior and market trends.\\n\\nAlgorithms Tried:\\nWe tested standard optimization techniques such as Multi-Armed Bandit algorithms and moved on to more advanced algorithms like Reinforcement Learning (RL). A Thompson Sampling-based approach was also implemented for balancing exploration and exploitation in ad bidding strategies.\\n\\nBest Performing Algorithm:\\nReinforcement Learning with Thompson Sampling performed best, allowing the system to learn and optimize bidding strategies on the fly based on user interactions and market fluctuations.\\n\\nKey Metrics:\\nROAS Improvement: 22%\\nAd Click-Through Rate (CTR) Increase: 18%\\nCost-Per-Click (CPC) Reduction: 15%\\nNext Steps:\\nFuture work includes refining user segmentation and integrating contextual data like user location and time of day to further improve targeting accuracy. We also aim to implement multi-channel ad optimization for a more comprehensive approach.\\n\\n')],\n",
       " 'answer': '?\\n\\nAI: Yes, we have worked on Project Titan, which focused on developing an autonomous driving system for self-driving cars. The primary objective was to navigate complex urban environments while ensuring the safety of passengers and pedestrians. We experimented with classical computer vision techniques like Convolutional Neural Networks (CNNs) for object detection and lane detection. Deep Reinforcement Learning (DRL) was also tested for decision-making. Ultimately,'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "#retriever = ...  # Your retriever\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a SINGLE [INST] block with system + human messages combined\n",
    "system_prompt = \"\"\"[INST] \n",
    "<<SYS>>\n",
    "You are a helpful assistant for Company XYZ. Answer questions based on the provided documents.\n",
    "<</SYS>>\n",
    "\n",
    "### Documents:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{input}\n",
    "\n",
    "If the answer is in the documents, use them. Otherwise, say \"The documents do not contain relevant information.\" [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Simplified prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt)])\n",
    "\n",
    "# Format with variables\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"Project documentation text here.\",\n",
    "    input=\"What is the project deadline?\"\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "#print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_stuff_documents_chain, create_retrieval_chain\n",
    "\n",
    "# Define a SINGLE [INST] block with system + human messages combined\n",
    "system_prompt = \"\"\"[INST] \n",
    "<<SYS>>\n",
    "You are a helpful assistant for Company XYZ. Answer questions based on the provided documents.\n",
    "<</SYS>>\n",
    "\n",
    "### Documents:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{input}\n",
    "\n",
    "If the answer is in the documents, use them. Otherwise, say \"The documents do not contain relevant information.\" [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Use from_template() instead of from_messages()\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Define the LLM-based document Q&A chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Define retrieval chain using retriever and Q&A chain\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# ✅ Define query before invoking the chain\n",
    "query = \"What is the project deadline?\"\n",
    "response = chain.invoke({\"input\": query})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_prompt = prompt.format(context=\"Project documentation text here.\", input=\"What is the project deadline?\")\n",
    "# print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = response[\"answer\"].strip()\n",
    "if response_text.startswith(\"?\"):\n",
    "    response_text = response_text[1:].strip()  # Remove leading \"?\" if present\n",
    "print(\"🔹 Answer:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"[INST] \n",
    "You are a helpful assistant for Company XYZ. Your task is to answer questions based on the provided project documents.\n",
    "\n",
    "### Documents:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{input}  \n",
    "If the documents contain relevant information, answer based only on them.\n",
    "If the documents do not provide an answer, say \"The documents do not contain relevant information.\"\n",
    "[/INST]\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),  # System prompt now uses {input}\n",
    "        (\"human\", \"{input}\"),      # Human message remains the same\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Format with consistent variable names\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"Project documentation text here.\",\n",
    "    input=\"What is the project deadline?\"\n",
    ")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a SINGLE [INST] block with system + human messages combined\n",
    "system_prompt = \"\"\"[INST] \n",
    "<<SYS>>\n",
    "You are a helpful assistant for Company XYZ. Answer questions based on the provided documents.\n",
    "<</SYS>>\n",
    "\n",
    "### Documents:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{input}\n",
    "\n",
    "If the answer is in the documents, use them. Otherwise, say \"The documents do not contain relevant information.\" [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Simplified prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt)])\n",
    "\n",
    "# Format with variables\n",
    "formatted_prompt = prompt.format(\n",
    "    context=\"Project documentation text here.\",\n",
    "    input=\"What is the project deadline?\"\n",
    ")\n",
    "\n",
    "print(formatted_prompt)dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
