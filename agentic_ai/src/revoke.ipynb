{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritkumar17/Desktop/R&D_Project/Agentic_AI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "/var/folders/00/rndss5vd6h9cdr24fwvnz6kw0000gp/T/ipykernel_71739/1026494264.py:15: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  custom_llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=200,  # Adjust for faster responses\n",
    "        temperature=0.1,  # Makes output more deterministic\n",
    "        top_p=0.95,  # Controls randomness\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "custom_llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritkumar17/Desktop/R&D_Project/Agentic_AI/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from agentic_ai.src.decision_making_agent_prompt import get_classification, RAG_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Which project used LSTMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_put \u001b[38;5;241m=\u001b[39m RAG_agent(test_query, \u001b[43mcustom_llm\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_llm' is not defined"
     ]
    }
   ],
   "source": [
    "out_put = RAG_agent(test_query, custom_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] \\n    You are a helpful assistant for Company XYZ. Your task is to answer questions based on the provided project documents.\\n    \\n    ### Documents:\\n    Project Nova: Predictive Maintenance for Industrial Equipment\\nProject Overview:\\nProject Nova was designed to build a predictive maintenance system for a large manufacturing firm. The goal was to predict equipment failures before they occurred, reducing downtime and improving operational efficiency. By leveraging IoT data from sensors embedded in machinery (e.g., vibration levels, temperature, motor speed), the project aimed to forecast when a piece of equipment would likely fail, allowing for timely intervention.\\n\\nAlgorithms Tried:\\nThe team experimented with a range of machine learning and deep learning models. Initially, we used statistical models like ARIMA to capture time-series trends. However, as the data complexity increased, we pivoted towards Random Forests and Gradient Boosting Machines (GBM). Ultimately, a Long Short-Term Memory (LSTM) neural network combined with feature engineering of sensor data provided the most accurate results, due to its ability to model sequential dependencies over time.\\n\\nBest Performing Algorithm:\\nThe LSTM model performed the best in capturing long-term dependencies in the time-series data, significantly improving prediction accuracy. By feeding real-time sensor data into the LSTM model, the system could predict the likelihood of equipment failure days in advance.\\n\\nKey Metrics:\\nFailure Prediction Accuracy: 95%\\nDowntime Reduction: 30%\\nCost Savings on Maintenance: 25%\\nNext Steps:\\nFuture development includes integrating external data sources such as weather conditions, which can affect machinery performance, and expanding the system to predict maintenance needs across multiple types of equipment. Moreover, the next phase will involve automating the generation of maintenance tickets based on predicted failures, further streamlining the process.\\n\\n\\n\\n\\n\\nProject Borealis: Air Quality Monitoring & Prediction\\nProject Overview:\\nProject Borealis was focused on developing a real-time air quality monitoring and prediction system for urban areas. The goal was to use sensor data to predict harmful pollution levels, allowing authorities to issue warnings and implement corrective actions.\\n\\nAlgorithms Tried:\\nWe initially applied simple time-series forecasting models like ARIMA, but moved on to more complex models like Random Forests and XGBoost due to the multifactorial nature of air quality. A hybrid model combining LSTM and CNN (Convolutional Neural Networks) provided the best results for both spatial and temporal dependencies.\\n\\nBest Performing Algorithm:\\nThe hybrid LSTM-CNN model worked best, as it could model both the time series nature of air quality data and the spatial dependencies between different monitoring stations.\\n\\nKey Metrics:\\nPrediction Accuracy: 91%\\nFalse Negative Rate: 6%\\nAverage Warning Lead Time: 1 hour\\nNext Steps:\\nNext steps include integrating real-time traffic and industrial emission data to further refine predictions. Additionally, the system will be scaled to cover larger regions and incorporate satellite-based pollution monitoring.\\n\\n\\n\\nProject Poseidon: Water Quality Prediction Using IoT Sensors\\nProject Overview:\\nProject Poseidon was aimed at developing a system to monitor and predict water quality in real-time using IoT sensors deployed in lakes and rivers. The project’s goal was to prevent water contamination by predicting potential pollutant levels and enabling timely intervention by environmental authorities.\\n\\nAlgorithms Tried:\\nInitially, linear regression and time-series models were used to predict pollutant levels based on historical data. We then shifted to Random Forests and Neural Networks to handle more complex, nonlinear relationships in the data. A recurrent neural network (RNN) combined with environmental feature extraction provided the best results.\\n\\nBest Performing Algorithm:\\nA Recurrent Neural Network (RNN) with LSTM layers provided the most accurate predictions by modeling time-dependent fluctuations in water quality levels effectively.\\n\\nKey Metrics:\\nPrediction Accuracy for Water Contaminants: 93%\\nFalse Negative Rate (Contaminants): 4%\\nResponse Time for Alerts: <10 minutes\\nNext Steps:\\nThe next steps include expanding the monitoring network by adding more IoT sensors and incorporating satellite imagery to enhance prediction accuracy. We are also working on integrating weather forecast data to anticipate how environmental changes might affect water quality.\\n\\n\\n\\n\\n\\nProject Horizon: Energy Consumption Forecasting for Smart Grids\\nProject Overview:\\nProject Horizon aimed at predicting energy consumption for a smart grid infrastructure to optimize energy distribution and reduce waste. By forecasting power demand accurately, the utility company could balance supply and demand, improving efficiency and reducing costs.\\n\\nAlgorithms Tried:\\nWe started with time-series models like ARIMA and Prophet, but moved to more sophisticated approaches such as Gradient Boosting Machines and Recurrent Neural Networks (RNNs). A Transformer-based model eventually provided the best performance in terms of scalability and accuracy.\\n\\nBest Performing Algorithm:\\nA Transformer model outperformed all other algorithms, allowing for both time-series prediction and capturing temporal patterns across multiple data sources (e.g., weather, industrial activity).\\n\\nKey Metrics:\\nForecast Accuracy: 94%\\nPeak Demand Reduction: 15%\\nEnergy Waste Reduction: 10%\\nNext Steps:\\nThe next steps include adding renewable energy sources to the model and expanding the system to predict not only demand but also optimal energy storage strategies for periods of low consumption.\\n\\n\\n\\n\\n\\nProject Apollo: Demand Forecasting for Retail Inventory Management\\nProject Overview:\\nProject Apollo was created to improve inventory management by developing a demand forecasting model for a large retail chain. The aim was to reduce stockouts and overstock situations by accurately predicting demand for various products in different store locations.\\n\\nAlgorithms Tried:\\nWe began with traditional time-series models like ARIMA and Holt-Winters, but these were limited in their capacity to model the complex dependencies in the data. Next, we used Gradient Boosting Machines (GBMs) and Neural Networks to capture seasonality and trends. A hybrid model combining GBMs and LSTM networks turned out to be the most effective.\\n\\nBest Performing Algorithm:\\nThe hybrid GBM-LSTM model performed best, as it could capture both short-term trends and long-term seasonality. This combination resulted in highly accurate demand forecasts.\\n\\nKey Metrics:\\nForecast Accuracy: 92%\\nStockout Reduction: 25%\\nOverstock Reduction: 20%\\nNext Steps:\\nThe next steps include integrating promotions and marketing campaigns into the demand forecasting model to account for spikes in demand. Additionally, the model will be expanded to cover multiple product categories, adjusting for differing demand cycles.\\n\\n\\n\\n    ### Question:\\n    Which project used LSTMs?\\n\\n    Provide a concise and accurate answer using only the information from the documents. If the answer is not found in the documents, respond with \"I don\\'t know.\"\\n    [/INST] Project Nova used LSTMs.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_put\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = out_put.split(\"[/INST]\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Nova used LSTMs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
